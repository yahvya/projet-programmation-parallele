An Introduction to Information Theory
          and Applications

 F. Bavaud       J.-C. Chappelier       J. Kohlas

      version 2.04 - 20050309 - UniFr course
2
Contents

1 Uncertainty and Information                                                                         9
  1.1 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    .   .   .   .   .   .   10
      1.1.1 Choice and Uncertainty . . . . . . . . . . . . . . . . .         .   .   .   .   .   .   10
      1.1.2 Choice with Known Probability . . . . . . . . . . . . .          .   .   .   .   .   .   18
      1.1.3 Conditional Entropy . . . . . . . . . . . . . . . . . . .        .   .   .   .   .   .   28
      1.1.4 Axiomatic Determination of Entropy . . . . . . . . . .           .   .   .   .   .   .   37
  1.2 Information And Its Measure . . . . . . . . . . . . . . . . . .        .   .   .   .   .   .   44
      1.2.1 Observations And Events . . . . . . . . . . . . . . . .          .   .   .   .   .   .   44
      1.2.2 Information and Questions . . . . . . . . . . . . . . .          .   .   .   .   .   .   51
      1.2.3 Mutual Information and Kullback-Leibler Divergence .             .   .   .   .   .   .   61
      1.2.4 Surprise, Entropy and Information . . . . . . . . . . .          .   .   .   .   .   .   69
      1.2.5 Probability as Information . . . . . . . . . . . . . . . .       .   .   .   .   .   .   73